{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment with regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokeniser = BertTokenizer.from_pretrained(\"bert-tiny\", local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here\n",
    "import torch\n",
    "from typing import Dict, Any, Tuple\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "# create a dataset class that inherits from torch.Dataset\n",
    "\n",
    "\n",
    "class EssayDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for handing training and test data.\n",
    "    TODO: Generalise to handle validation split as well.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        tokeniser: BertTokenizer,\n",
    "        target_label=None,\n",
    "        max_length: int = 512,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        df (pd.DataFrame): Dataframe of train or test dataset.\n",
    "        tokeniser (BertTokenizer): Pre-trained tokenizer\n",
    "        target_label (string) : The label corresponding to our scores.\n",
    "        max_length(int, optional):\n",
    "        \"\"\"\n",
    "        self.data = df\n",
    "        self.tokenizer = tokeniser\n",
    "        self.max_length = max_length\n",
    "        self.target_label = target_label\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns the number of samples.\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Dict[str, Tensor]:\n",
    "        \"\"\"\n",
    "        Tokenises the essay and returns input or input+label depending on\n",
    "        whether self.target_label is None.\n",
    "\n",
    "        Returns:\n",
    "            data_dict (Dict[str, Tensor]): Dictionary of tokenised essay, attention_mask and score\n",
    "\n",
    "        \"\"\"\n",
    "        essay = str(self.data.iloc[index][\"content\"])  # get essay text\n",
    "        # score = int(self.data.iloc[index][\"score\"]) - 1 # convert to 0-based index\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            essay,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        data_dict = {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
    "        }\n",
    "\n",
    "        # this generalises the EssayDataset class to work with test data as well\n",
    "        # we also need to check that the target_label is indeed \"score\"\n",
    "        # obviously this could be generalised further.\n",
    "        if self.target_label is not None:\n",
    "            # convert to 0-based index\n",
    "            data_dict[\"score\"] = torch.tensor(int(self.data.iloc[index][\"score\"]) - 1)\n",
    "\n",
    "        return data_dict\n",
    "\n",
    "\n",
    "def split_essay_data(full_dataset) -> Tuple[Dataset, Dataset]:\n",
    "    \"\"\"\n",
    "    Split essay data for to check performance on held out validation set.\n",
    "\n",
    "    Args:\n",
    "        full_dataset (Dataset): The full dataset that we intend to split.\n",
    "\n",
    "    Returns:\n",
    "        train_dataset (Dataset)\n",
    "        val_dataset (Dataset)\n",
    "\n",
    "    \"\"\"\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "\n",
    "    # set manual seed to ensure reproducibility\n",
    "    # torch.manual_seed(42)\n",
    "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/cleaned_dataset.csv\")\n",
    "\n",
    "full_dataset = EssayDataset(train_df, tokeniser, target_label=\"score\")\n",
    "\n",
    "train_dataset, val_dataset = split_essay_data(full_dataset)\n",
    "\n",
    "# from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "#\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.nn import CrossEntropyLoss, Module\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Optimizer, AdamW\n",
    "from numpy import mean\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def get_accuracy(preds, labels):\n",
    "    \"\"\"\n",
    "    TODO: docstrings\n",
    "    \"\"\"\n",
    "    correct = (preds == labels).sum().item()\n",
    "    total = labels.size(0)\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "def get_rmse(outputs, labels):\n",
    "    \"\"\"\n",
    "    TODO: docstrings\n",
    "    \"\"\"\n",
    "    return torch.sqrt(F.mse_loss(outputs, labels)).item()\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "    model: Module, val_loader: DataLoader, loss_fn: CrossEntropyLoss\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    TODO: docstrings\n",
    "    \"\"\"\n",
    "    # Put model in evaluation mode. So we do not drink statistics from validation set. (Take statistics from training)\n",
    "    model.eval()\n",
    "\n",
    "    # add to metrics dictionary\n",
    "    metrics = {\"loss\": [], \"rmse\": []}\n",
    "\n",
    "    # no need to store gradients\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"]\n",
    "\n",
    "            attention_mask = batch[\"attention_mask\"]\n",
    "\n",
    "            labels = batch[\"score\"]\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "\n",
    "            # add to metrics dictionary\n",
    "            metrics[\"loss\"].append(loss.item())\n",
    "            metrics[\"rmse\"].append(get_rmse(outputs, labels))\n",
    "\n",
    "    return {key: mean(val) for key, val in metrics.items()}\n",
    "\n",
    "\n",
    "# could also use BCELoss - convert to one-hot encoding?\n",
    "def train_model(\n",
    "    model: Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    optimiser: Optimizer,\n",
    "    loss_fn: CrossEntropyLoss,\n",
    "    epochs: int = 1,\n",
    ") -> Module:\n",
    "    \"\"\"\n",
    "    TODO: Improve docstrings. This is just a skeleton.\n",
    "\n",
    "    Train a model on essay text with scores as labels.\n",
    "\n",
    "    Args:\n",
    "        model (Module) :\n",
    "        train_loader (DataLoader) :\n",
    "        val_loader (DataLoader) : (UNUSED) - this would be used here if we trained for multiple epochs.\n",
    "        optimiser (Optimiser) :\n",
    "        loss_fn (CrossEntropyLoss) :\n",
    "        epochs (int) :\n",
    "\n",
    "    Returns:\n",
    "        Trained model\n",
    "\n",
    "    \"\"\"\n",
    "    # Prepare model for training. This turns certain layers on and tells batch\n",
    "    # norm layers to use incoming statistics and let the contribute to their memory.\n",
    "    model.train()\n",
    "\n",
    "    metrics = {\"loss\": [], \"rmse\": []}\n",
    "\n",
    "    # for illustration only (we only train on one epoch).\n",
    "    for epoch in range(epochs):\n",
    "        for iter, batch in enumerate(train_loader):\n",
    "            # zero the gradients (otherwise gradients accumulate)\n",
    "            optimiser.zero_grad()\n",
    "\n",
    "            input_ids = batch[\"input_ids\"]\n",
    "            # the attention mask tells us which tokens are real words and which are padding.\n",
    "\n",
    "            attention_mask = batch[\"attention_mask\"]\n",
    "            labels = batch[\"score\"].float()\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            # compute loss\n",
    "            # outputs.logits is the raw output of the model. It has shape (batch_size, num_labels)\n",
    "            # labels is the true label of the data. It has shape (batch_size)\n",
    "            # internally CE loss does the following:\n",
    "            # loss = -log(softmax(logits)[label])\n",
    "            loss = loss_fn(outputs.logits.squeeze(-1), labels)\n",
    "\n",
    "            # compute gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # nudge parameters in direction of steepest descent\n",
    "            optimiser.step()\n",
    "\n",
    "            # loss is tensor - get value using item()\n",
    "            metrics[\"loss\"].append(loss.item())\n",
    "            metrics[\"rmse\"].append(get_rmse(outputs.logits.squeeze(-1), labels))\n",
    "\n",
    "            # debugging\n",
    "            # print(iter, metrics[\"accuracy\"][-1], loss.item())\n",
    "            # visualise every 10 steps (10 * batch_size) = 160 samples per print.\n",
    "            if iter % 10 == 0:\n",
    "                print(\"step: \", iter, \", batch loss: \", loss.item())\n",
    "\n",
    "    # just output train averages for now - we'll get validation loss / accuracy next.\n",
    "    print(\"Train averages \", {key: mean(val) for key, val in metrics.items()})\n",
    "\n",
    "    # return fine-tuned model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertConfig, BertForSequenceClassification\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# class BertForEssayRegression(nn.Module):\n",
    "#     def __init__(self, pretrained_model_name=\"bert-tiny\"):\n",
    "#         super().__init__()\n",
    "#         self.bert = BertModel.from_pretrained(\n",
    "#             pretrained_model_name, local_files_only=True\n",
    "#         )\n",
    "#         self.regressor = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         # Take the pooled output (CLS token)\n",
    "#         cls_output = outputs.pooler_output  # shape: (batch_size, hidden_dim)\n",
    "#         score = self.regressor(cls_output).squeeze(-1)  # shape: (batch_size,)\n",
    "#         return score\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-tiny\",\n",
    "    num_labels=1,\n",
    "    problem_type=\"regression\",  # explicitly tells it it's regression\n",
    "    local_files_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  0 , batch loss:  4.303873062133789\n",
      "step:  10 , batch loss:  3.8187777996063232\n",
      "step:  20 , batch loss:  2.064824342727661\n",
      "step:  30 , batch loss:  1.2988307476043701\n",
      "step:  40 , batch loss:  0.8945863842964172\n",
      "step:  50 , batch loss:  1.1415860652923584\n",
      "step:  60 , batch loss:  0.7110911011695862\n",
      "step:  70 , batch loss:  0.6081348061561584\n",
      "step:  80 , batch loss:  0.5337756872177124\n",
      "Train averages  {'loss': 1.5377885425506637, 'rmse': 1.1720104085844616}\n"
     ]
    }
   ],
   "source": [
    "# we treat this as a very simple classification task - even though we could potentially make use of ordinality in loss function.\n",
    "# model = BertForEssayRegression()\n",
    "optimiser: Optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "# loss = loss_fn(outputs, labels)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "model = train_model(model, train_loader, val_loader, optimiser, loss_fn, epochs=1)\n",
    "# model.save_pretrained(\"bert_tiny_finetuned\")\n",
    "torch.save(model.state_dict(), \"model-trained-regression.pth\")\n",
    "\n",
    "# torch.save(model.state_dict(), \"bert_tiny_finetuned.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rounded accuracy on val set: 0.6366\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch in val_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"score\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        preds = torch.round(outputs)  # <--- ROUND TO NEAREST INTEGER\n",
    "\n",
    "        all_preds.append(preds.cpu())\n",
    "        all_labels.append(labels.cpu())\n",
    "\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    accuracy = (all_preds == all_labels).float().mean().item()\n",
    "    print(f\"Rounded accuracy on val set: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
