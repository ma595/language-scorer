{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are developing an automated language scorer which will be able to able to predict a score by analyzing text essays content.\n",
    "\n",
    "Given a dataset of essays and their corresponding scores on the scale of 1-5, we are going to fine-tune a small Transformer model using PyTorch that can accurately predict the score of an essay.\n",
    " \n",
    "### Environment\n",
    "- For this coding test a 2-core CPU and 16GB of RAM is available\n",
    "- Git is available to version files, any part of the code given can be modified\n",
    "- Any other external tool can be used for this test\n",
    "- Expect to take 2 to 3 hours to complete these exercise\n",
    "- There is a 6 hours time limit for your submission\n",
    "\n",
    "### Files and folders\n",
    "- `train.csv`, the training dataset\n",
    "- `test.csv`, the testing dataset\n",
    "- `bert-tiny`, the pre-trained model to fine-tune and its related files\n",
    "- `sample-submissions.csv`, an example of how your `submissions.csv` file should be formatted\n",
    " \n",
    "### Deliverables\n",
    "\n",
    "1. 'submissions.csv': For each record in the test set `test.csv`, predict the value of the 'score' variable. Submit a CSV file with a header row and one row per test entry. The file `submissions.csv` should have exactly 2 columns:\n",
    "content and score (1 -5).\n",
    "\n",
    "> The model will be tested on a different set of essays from the training set.\n",
    "\n",
    "2. Well commented Jupyter notebook including cell outputs and answers to Questions 1-3.\n",
    "\n",
    "> Annotate the notebook with code comments and markdowns. A person should be able to read the notebook and understand the steps taken as well as the reasoning behind them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-03-28 15:22:27--  https://hr-projects-assets-prod.s3.amazonaws.com/6cj60qp1cd9/97ebab92d945fd74ec334ec7d7aecac0/pytorch_model.bin\n",
      "Resolving hr-projects-assets-prod.s3.amazonaws.com (hr-projects-assets-prod.s3.amazonaws.com)... 52.217.234.217, 52.216.54.193, 3.5.28.69, ...\n",
      "Connecting to hr-projects-assets-prod.s3.amazonaws.com (hr-projects-assets-prod.s3.amazonaws.com)|52.217.234.217|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 17756393 (17M) [binary/octet-stream]\n",
      "Saving to: ‘bert-tiny/pytorch_model.bin’\n",
      "\n",
      "pytorch_model.bin   100%[===================>]  16.93M  3.30MB/s    in 5.2s    \n",
      "\n",
      "2025-03-28 15:22:36 (3.29 MB/s) - ‘bert-tiny/pytorch_model.bin’ saved [17756393/17756393]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Install libraries\n",
    "# ! pip3 install transformers==4.26.0 torch pandas datasets\n",
    "\n",
    "# Download base model\n",
    "! wget -P bert-tiny/ https://hr-projects-assets-prod.s3.amazonaws.com/6cj60qp1cd9/97ebab92d945fd74ec334ec7d7aecac0/pytorch_model.bin\n",
    "\n",
    "# Ignore model folder in git\n",
    "! echo \"bert-tiny/\" >> .gitignore\n",
    "\n",
    "# To install packages that are not installed by default, uncomment the next line\n",
    "# and replace <package list> with a list of needed packages.\n",
    "\n",
    "#! pip3 install <package list>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matt/work/fine-tuning/venv3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/matt/work/fine-tuning/venv3/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "BertTokenizer.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\").save_pretrained(\n",
    "    \"bert-tiny\"\n",
    ")\n",
    "tokeniser = BertTokenizer.from_pretrained(\"bert-tiny\", local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matt/work/fine-tuning/venv-11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Load libraries, add more as required\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score Distribution:\n",
      " score\n",
      "1     15\n",
      "2    154\n",
      "3    702\n",
      "4    845\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3]),\n",
       " [Text(0, 0, '1'), Text(1, 0, '2'), Text(2, 0, '3'), Text(3, 0, '4')])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_df = pd.read_csv(\"data/cleaned_dataset.csv\")\n",
    "score_counts = train_df[\"score\"].value_counts().sort_index()\n",
    "\n",
    "print(\"Score Distribution:\\n\", score_counts)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "score_counts.plot(kind=\"bar\")\n",
    "plt.xlabel(\"Score\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of Essay Scores\")\n",
    "plt.xticks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Certain materials being removed from libraries...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Write a persuasive essay to a newspaper reflec...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Do you think that libraries should remove cert...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In @DATE1's world, there are many things found...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In life you have the 'offensive things'. The l...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1711</th>\n",
       "      <td>Libraries have a variety of material from book...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1712</th>\n",
       "      <td>I do not think that materials, such as books, ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1713</th>\n",
       "      <td>Yes we should keep the books,music,movies,an m...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1714</th>\n",
       "      <td>I do believe that  book, magazines, music, mov...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1715</th>\n",
       "      <td>Different Then Everyone Else     @CAPS1 do peo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1716 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                content  score\n",
       "0     Certain materials being removed from libraries...      4\n",
       "1     Write a persuasive essay to a newspaper reflec...      1\n",
       "2     Do you think that libraries should remove cert...      3\n",
       "3     In @DATE1's world, there are many things found...      4\n",
       "4     In life you have the 'offensive things'. The l...      4\n",
       "...                                                 ...    ...\n",
       "1711  Libraries have a variety of material from book...      3\n",
       "1712  I do not think that materials, such as books, ...      3\n",
       "1713  Yes we should keep the books,music,movies,an m...      2\n",
       "1714  I do believe that  book, magazines, music, mov...      4\n",
       "1715  Different Then Everyone Else     @CAPS1 do peo...      2\n",
       "\n",
       "[1716 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_df.iloc[0]\n",
    "# drop index\n",
    "# train_df = train_df.drop(columns=[\"Unnamed: 0\"])\n",
    "train_df\n",
    "# keep only content and score\n",
    "train_df = train_df[[\"content\", \"score\"]]\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# distribution of essay lengths\n",
    "train_df[\"text_length\"] = train_df[\"content\"].apply(lambda x: len(str(x).split()))\n",
    "plt.figure(figsize=(8, 5))\n",
    "train_df[\"text_length\"].hist(bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Will a model with a maximum sequence of 512 tokens be useful?\n",
    "\n",
    "Yes, but a few caveats. Some essays exceed the 512 token limit. There will be some truncation - long essays - of which there are only a few, will affect model performance as some information is lost. One approach is to split long essays into overlapping chunks and then process each separaely - we can then make multiple predictions per essay and then average. It's also useful to bear in mind that BERT tokenization does not correspond to word count. 1 word can be split up into multiple tokens (1.3+). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2. Divide the training dataset in 80% used for training and 20% for validation. Train a neural network, that can predict the score of the essays. It needs to be based on BERT-tiny and fine-tuned with the available dataset contained in train.csv. Show that your model accuracy on the validation part of the dataset is 75% or more. Training can take a few minutes, we recommend using a single epoch. \n",
    "\n",
    "Suggested steps:\n",
    "- Create two PyTorch DataLoaders, training and validation from train.csv\n",
    "- Load the base model for training\n",
    "- Train for a single epoch (this computation takes about 15 minutes)\n",
    "- Check that predictions on validation dataset are sensible\n",
    "- Save fine-tuned model including optimizer states.\n",
    "\n",
    "Code for loading the dataset in a DataFrame and the tokenizer and model is given below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Dataset classes for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Dict, Any, Tuple\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "# create a dataset class that inherits from torch.Dataset\n",
    "class EssayDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for handing training and test data.\n",
    "    TODO: Generalise to handle validation split as well.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        tokeniser: BertTokenizer,\n",
    "        target_label=None,\n",
    "        max_length: int = 512,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        df (pd.DataFrame): Dataframe of train or test dataset.\n",
    "        tokeniser (BertTokenizer): Pre-trained tokenizer\n",
    "        target_label (string) : The label corresponding to our scores.\n",
    "        max_length(int, optional):\n",
    "        \"\"\"\n",
    "        self.data = df\n",
    "        self.tokenizer = tokeniser\n",
    "        self.max_length = max_length\n",
    "        self.target_label = target_label\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns the number of samples.\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Dict[str, Tensor]:\n",
    "        \"\"\"\n",
    "        Tokenises the essay and returns input or input+label depending on\n",
    "        whether self.target_label is None.\n",
    "\n",
    "        Returns:\n",
    "            data_dict (Dict[str, Tensor]): Dictionary of tokenised essay, attention_mask and score\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # performance implications of this? \n",
    "        \n",
    "        essay = str(self.data.iloc[index][\"content\"])  # get essay text\n",
    "        # score = int(self.data.iloc[index][\"score\"]) - 1 # convert to 0-based index\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            essay,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        data_dict = {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
    "        }\n",
    "\n",
    "        # this generalises the EssayDataset class to work with test data as well\n",
    "        # we also need to check that the target_label is indeed \"score\"\n",
    "        # obviously this could be generalised further.\n",
    "        if self.target_label is not None:\n",
    "            # convert to 0-based index\n",
    "            data_dict[\"score\"] = torch.tensor(int(self.data.iloc[index][\"score\"]) - 1)\n",
    "\n",
    "        return data_dict\n",
    "\n",
    "\n",
    "def split_essay_data(full_dataset) -> Tuple[Dataset, Dataset]:\n",
    "    \"\"\"\n",
    "    Split essay data for to check performance on held out validation set.\n",
    "\n",
    "    Args:\n",
    "        full_dataset (Dataset): The full dataset that we intend to split.\n",
    "\n",
    "    Returns:\n",
    "        train_dataset (Dataset)\n",
    "        val_dataset (Dataset)\n",
    "\n",
    "    \"\"\"\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "\n",
    "    # set manual seed to ensure reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create datasets and train, val data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "tokeniser = BertTokenizer.from_pretrained(\"bert-tiny\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-tiny\", num_labels=4)\n",
    "\n",
    "train_df = pd.read_csv(\"data/cleaned_dataset.csv\")\n",
    "\n",
    "full_dataset = EssayDataset(train_df, tokeniser, target_label=\"score\")\n",
    "\n",
    "train_dataset, val_dataset = split_essay_data(full_dataset)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "#\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  test EssayDataset - TODO: provide a robust set of tests here.\n",
    "\n",
    "# test __getitem__\n",
    "print(train_dataset[0])\n",
    "\n",
    "# train_df.iloc[0] # -> []\n",
    "\n",
    "# print(train_df.iloc[0])\n",
    "# test __len__\n",
    "# print(len(full_dataset))\n",
    "\n",
    "# print(\"train dataset length \", len(train_dataset))\n",
    "# print(\"val dataset length \", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-tiny\", local_files_only=True, num_labels=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.nn import CrossEntropyLoss, Module\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Optimizer, AdamW\n",
    "from numpy import mean\n",
    "\n",
    "\n",
    "def get_accuracy(preds, labels):\n",
    "    \"\"\"\n",
    "    TODO: docstrings\n",
    "    \"\"\"\n",
    "    correct = (preds == labels).sum().item()\n",
    "    total = labels.size(0)\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "    model: Module, val_loader: DataLoader, loss_fn: CrossEntropyLoss\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    TODO: docstrings\n",
    "    \"\"\"\n",
    "    # Put model in evaluation mode. So we do not drink statistics from validation set. (Take statistics from training\n",
    "    # uses running stats instead of batch stats. Also turns off dropout layers.\n",
    "    model.eval()\n",
    "\n",
    "    # add to metrics dictionary\n",
    "    metrics = {\"loss\": [], \"accuracy\": []}\n",
    "\n",
    "    # no need to store gradients\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"]\n",
    "\n",
    "            attention_mask = batch[\"attention_mask\"]\n",
    "\n",
    "            labels = batch[\"score\"]\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            # outputs.loits is the raw output of the model. It has shape (batch_size, num_labels)\n",
    "\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "\n",
    "            # accumulate total loss over entire batch\n",
    "            preds = torch.argmax(outputs.logits, dim=1) # (batch_size,)\n",
    "\n",
    "            # add to metrics dictionary\n",
    "            metrics[\"loss\"].append(loss.item())\n",
    "            metrics[\"accuracy\"].append(get_accuracy(preds, labels))\n",
    "\n",
    "    # dictionary comprehension to get mean of each metric\n",
    "    return {key: mean(val) for key, val in metrics.items()}\n",
    "\n",
    "\n",
    "# could also use BCELoss - convert to one-hot encoding?\n",
    "def train_model(\n",
    "    model: Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    optimiser: Optimizer,\n",
    "    loss_fn: CrossEntropyLoss,\n",
    "    epochs: int = 1,\n",
    ") -> Module:\n",
    "    \"\"\"\n",
    "    TODO: Improve docstrings. This is just a skeleton.\n",
    "\n",
    "    Train a model on essay text with scores as labels.\n",
    "\n",
    "    Args:\n",
    "        model (Module) :\n",
    "        train_loader (DataLoader) :\n",
    "        val_loader (DataLoader) : (UNUSED) - this would be used here if we trained for multiple epochs.\n",
    "        optimiser (Optimiser) :\n",
    "        loss_fn (CrossEntropyLoss) :\n",
    "        epochs (int) :\n",
    "\n",
    "    Returns:\n",
    "        Trained model\n",
    "\n",
    "    \"\"\"\n",
    "    # Prepare model for training. This turns certain layers on and tells batch\n",
    "    # norm layers to use incoming statistics and let the contribute to their memory.\n",
    "    model.train()\n",
    "\n",
    "    metrics = {\"loss\": [], \"accuracy\": []}\n",
    "\n",
    "    # for illustration only (we only train on one epoch).\n",
    "    for epoch in range(epochs):\n",
    "        for iter, batch in enumerate(train_loader):\n",
    "            # zero the gradients (otherwise gradients accumulate)\n",
    "            optimiser.zero_grad()\n",
    "\n",
    "            input_ids = batch[\"input_ids\"]\n",
    "            # the attention mask tells us which tokens are real words and which are padding.\n",
    "\n",
    "            attention_mask = batch[\"attention_mask\"]\n",
    "            labels = batch[\"score\"]\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            # compute loss\n",
    "            # outputs.logits is the raw output of the model. It has shape (batch_size, num_labels)\n",
    "            # labels is the true label of the data. It has shape (batch_size)\n",
    "            # internally CE loss does the following:\n",
    "            # loss = -log(softmax(logits)[label])\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "\n",
    "            # compute gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # nudge parameters in direction of steepest descent\n",
    "            optimiser.step()\n",
    "\n",
    "            # compute accuracy on batch basis\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "            # loss is tensor - get value using item()\n",
    "            metrics[\"loss\"].append(loss.item())\n",
    "            metrics[\"accuracy\"].append(get_accuracy(preds, labels))\n",
    "\n",
    "            # debugging\n",
    "            # print(iter, metrics[\"accuracy\"][-1], loss.item())\n",
    "            # visualise every 10 steps (10 * batch_size) = 160 samples per print.\n",
    "            if iter % 10 == 0:\n",
    "                print(\"step: \", iter, \", batch loss: \", loss.item())\n",
    "\n",
    "    # just output train averages for now - we'll get validation loss / accuracy next.\n",
    "    print(\"Train averages \", {key: mean(val) for key, val in metrics.items()})\n",
    "\n",
    "    # return fine-tuned model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model and call train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  0 , batch loss:  1.4228320121765137\n",
      "step:  10 , batch loss:  1.1591801643371582\n",
      "step:  20 , batch loss:  1.1523183584213257\n",
      "step:  30 , batch loss:  0.9721835255622864\n",
      "step:  40 , batch loss:  0.9102092385292053\n",
      "step:  50 , batch loss:  0.9839202165603638\n",
      "step:  60 , batch loss:  0.9565108418464661\n",
      "step:  70 , batch loss:  0.8535860776901245\n",
      "step:  80 , batch loss:  1.2091543674468994\n",
      "Train averages  {'loss': 1.053316436534704, 'accuracy': 0.5135658914728682}\n"
     ]
    }
   ],
   "source": [
    "# main loop\n",
    "# we treat this as a very simple classification task - even though we could potentially make use of ordinality in loss function.\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-tiny\", local_files_only=True, num_labels=4\n",
    ")\n",
    "optimiser: Optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "loss_fn: CrossEntropyLoss = CrossEntropyLoss()\n",
    "\n",
    "model = train_model(model, train_loader, val_loader, optimiser, loss_fn, epochs=1)\n",
    "model.save_pretrained(\"bert_tiny_finetuned\")\n",
    "torch.save(optimiser.state_dict(), \"optimiser-fine.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make sure the predictions are sensible "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:  tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 3, 3, 2, 2])\n",
      "Actual:  tensor([3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 1, 2, 2, 1, 1, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-0.9942, -0.4938,  0.1620,  1.1190],\n",
       "        [-1.2137, -0.4963,  0.6364,  0.6838],\n",
       "        [-0.9766, -0.5466,  0.4540,  1.0833],\n",
       "        [-1.1787, -0.6540,  0.4394,  1.0347],\n",
       "        [-1.3606, -0.5886,  0.6200,  0.9516],\n",
       "        [-1.3037, -0.4795,  0.6281,  0.8999],\n",
       "        [-1.1284, -0.5392,  0.2702,  1.1903],\n",
       "        [-1.1455, -0.6382,  0.3180,  1.2532],\n",
       "        [-1.2099, -0.6484,  0.7941,  0.8019],\n",
       "        [-1.0874, -0.5379,  0.3874,  1.0748],\n",
       "        [-0.9370, -0.3066,  0.6743,  0.3693],\n",
       "        [-1.1064, -0.4101,  0.7825,  0.2802],\n",
       "        [-1.2708, -0.5810,  0.5323,  1.1179],\n",
       "        [-0.7910, -0.4053,  0.5281,  0.6111],\n",
       "        [-1.0750, -0.3375,  0.6678,  0.6633],\n",
       "        [-1.3713, -0.5644,  0.8123,  0.7811]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a single item from the validation set and check the model's prediction\n",
    "# we will use this to check the model's performance on a single item.\n",
    "val_iter = iter(val_loader)\n",
    "val_batch = next(val_iter)\n",
    "input_ids = val_batch[\"input_ids\"]\n",
    "attention_mask = val_batch[\"attention_mask\"]\n",
    "labels = val_batch[\"score\"]\n",
    "\n",
    "outputs = model(input_ids, attention_mask=attention_mask)\n",
    "preds = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "print(\"Predicted: \", preds)\n",
    "print(\"Actual: \", labels)\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.8755743286826394  accuracy:  0.6505681818181818\n"
     ]
    }
   ],
   "source": [
    "# now evaluate the model on unseen validation data\n",
    "metrics = evaluate_model(model, val_loader, loss_fn)\n",
    "print(\"loss: \", metrics[\"loss\"], \" accuracy: \", metrics[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore evaluation a little more\n",
    "\n",
    "- Per class accuracy\n",
    "- Confusion matrix\n",
    "- Precision, recall, F1 score\n",
    "\n",
    "\n",
    "The performance below is terrible on label = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"score\"]\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        preds = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "# Concatenate everything\n",
    "all_preds = torch.cat(all_preds)\n",
    "all_labels = torch.cat(all_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(False)\n"
     ]
    }
   ],
   "source": [
    "all_labels\n",
    "\n",
    "# check if all_labels tensor contains a 0 or 4:\n",
    "print(torch.any(all_labels == 4)) # FALSE\n",
    "print(torch.any(all_labels == 0)) # TRUE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 1, 2, 2, 1, 1, 2, 2, 2, 2, 2, 3, 2, 3, 3,\n",
       "        3, 2, 3, 2, 2, 2, 2, 3, 2, 2, 3, 2, 2, 2, 2, 2, 3, 3, 3, 2, 3, 2, 2, 3,\n",
       "        3, 2, 2, 2, 2, 1, 2, 2, 2, 3, 3, 3, 2, 2, 3, 3, 3, 3, 2, 3, 1, 2, 2, 3,\n",
       "        2, 3, 3, 3, 2, 1, 2, 2, 3, 3, 3, 3, 2, 3, 2, 1, 3, 3, 3, 3, 3, 2, 3, 2,\n",
       "        1, 3, 2, 3, 3, 3, 2, 2, 2, 2, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 1, 3, 3, 2,\n",
       "        2, 2, 1, 3, 2, 2, 1, 2, 1, 3, 3, 2, 3, 3, 2, 2, 3, 3, 2, 0, 3, 2, 3, 1,\n",
       "        1, 2, 3, 3, 2, 2, 2, 3, 3, 3, 2, 3, 3, 3, 2, 2, 3, 3, 0, 2, 2, 3, 1, 3,\n",
       "        2, 3, 3, 2, 2, 3, 2, 2, 2, 3, 3, 2, 3, 3, 3, 2, 2, 3, 3, 2, 0, 2, 2, 3,\n",
       "        3, 3, 1, 3, 2, 2, 3, 2, 2, 3, 2, 1, 3, 2, 3, 2, 3, 2, 3, 2, 2, 2, 3, 3,\n",
       "        3, 2, 2, 3, 2, 2, 3, 3, 3, 1, 3, 2, 3, 3, 3, 3, 2, 3, 2, 3, 3, 3, 2, 3,\n",
       "        2, 3, 3, 2, 2, 3, 3, 2, 3, 3, 3, 3, 2, 2, 3, 2, 2, 3, 3, 2, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 1, 3, 3, 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3,\n",
       "        2, 3, 2, 3, 3, 2, 3, 2, 3, 1, 2, 3, 2, 3, 3, 2, 2, 3, 3, 2, 3, 3, 2, 2,\n",
       "        1, 3, 3, 2, 3, 2, 2, 2, 3, 2, 3, 2, 3, 2, 2, 3, 2, 2, 2, 3, 3, 2, 3, 3,\n",
       "        3, 3, 2, 3, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.0, 2: 0.5416666666666666, 3: 0.8409090909090909, 4: 0.0}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = 4  \n",
    "per_class_acc = {}\n",
    "\n",
    "for cls in range(0, num_classes):\n",
    "    mask = all_labels == cls \n",
    "    correct = (all_preds[mask] == cls).sum().item()\n",
    "    total = mask.sum().item()\n",
    "    per_class_acc[cls] = correct / total if total > 0 else 0.0\n",
    "\n",
    "\n",
    "per_class_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is heavy class imbalance with this dataset. Possible things to explore:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I suspect that OrdinalCrossEntropyLoss might be better for this sort of task. We can do:\n",
    "```python\n",
    "from torch_ordinal import OrdinalCrossEntropyLoss\n",
    "loss_fn = OrdinalCrossEntropyLoss(...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3. Use the model you created to run inference on each example of the test dataset and save all predictions in submissions.csv file.\n",
    "\n",
    "Suggested steps:\n",
    "\n",
    "- Create one PyTorch DataLoader, test from test.csv\n",
    "- Load your fine-tuned model for evaluation\n",
    "- Generate predictions for all datum in test.csv\n",
    "- Save results in submissions.csv in format defined above.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model from disk\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert_tiny_finetuned\", local_files_only=True, num_labels=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_loader) -> List[int]:\n",
    "    \"\"\"\n",
    "    Run inference on a test dataset using the trained model.\n",
    "\n",
    "    Args:\n",
    "        model (Module). The fine-tuned model for classification.\n",
    "        test_loader (DataLoader): DataLoader containing the test dataset.\n",
    "\n",
    "    Returns:\n",
    "        List[int]: A list of predicted class labels in the range [1,5]\n",
    "    \"\"\"\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    # do not store gradients during prediction\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch[\"input_ids\"]\n",
    "            attention_mask = batch[\"attention_mask\"]\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            # remap predictions to [1..5] (before it was [0..4])\n",
    "            preds = torch.argmax(outputs.logits, dim=1).numpy() + 1\n",
    "            # could potentially do this more efficiently.\n",
    "            for pred in preds.tolist():\n",
    "                predictions.append(pred)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch to eval mode\n",
    "model.eval()\n",
    "\n",
    "# initialise the train dataset\n",
    "test_df = pd.read_csv(\"data/test.csv\")\n",
    "test_dataset = EssayDataset(test_df, tokeniser, target_label=None)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "# make predictions on test set\n",
    "test_predictions = predict(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now save predictions to submissions.csv in the appropriate format\n",
    "\n",
    "test_df = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "test_df[\"score\"] = test_predictions\n",
    "test_df[[\"content\", \"score\"]].to_csv(\"data/submissions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    actual_file = \"./data/sample-submissions.csv\"\n",
    "    pred_file = \"./data/submissions.csv\"\n",
    "\n",
    "    actual_data = pd.read_csv(actual_file)\n",
    "    actuals = np.array(actual_data[\"score\"].tolist()[:10])\n",
    "\n",
    "    pred_data = pd.read_csv(pred_file)\n",
    "    preds = np.array(pred_data[\"score\"].tolist()[:10])\n",
    "\n",
    "    metric = 1 - np.mean(np.abs(actuals - preds) / actuals)\n",
    "\n",
    "    if metric < 0 or metric > 1:\n",
    "        metric = 0\n",
    "\n",
    "    print(f\"Format valid, FS_SCORE:{metric * 100} %\")\n",
    "except Exception as e:\n",
    "    print(\"Format invalid: \", e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
